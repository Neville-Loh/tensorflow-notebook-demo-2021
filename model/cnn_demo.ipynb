{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cnn-demo.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hplrEsbiugPE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWkrjmop2GwG"
      },
      "source": [
        "#Convolution Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTu9mETC2wtr"
      },
      "source": [
        "### Importing ilbraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O06X4q0L23gj"
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "UuTo9lDB3F7j",
        "outputId": "89b0af89-efbb-44ab-d4df-a284f4f8dfb3"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3w8DdkeK2Thm"
      },
      "source": [
        "## 1 - Data Preprocessing "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uo6FBC883NkN"
      },
      "source": [
        "### Preprocessing the Training Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Im_4aksa3RDY"
      },
      "source": [
        "## Transformation, avoid overfitting. will get very high accracy\n",
        "## This is going to apply some simple transformation/ rotation to the image / rotations/ zoom in and out flips/ \n",
        "## image augmentation\n",
        "## https://keras.io/api/preprocessing/\n",
        "\n",
        "## Create object \n",
        "\n",
        "# Open slides \n",
        "# rescale = feature scaling 1part out of 255\n",
        "# shear range = shear x +- 20%\n",
        "# zoom range = zoom +- 20%\n",
        "# horizontal_flip = flip the image randomly \n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True)\n",
        "\n",
        "## .flow_from_directory basically picks up your image set there\n",
        "## path to the training set folder\n",
        "## target_size, is the target size of the result image\n",
        "## 150 by 150 kinda too large\n",
        "## 64, 64 is quite good and fast\n",
        "\n",
        "\n",
        "## class mode is either binary/categorical\n",
        "## Can go to \n",
        "training_set = train_datagen.flow_from_directory(\n",
        "        'data/train',\n",
        "        target_size=(64, 64),\n",
        "        batch_size=32,\n",
        "        class_mode='binary')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcY6kD4o3UEK"
      },
      "source": [
        "### Preprocessing the Test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eADUqYfRwl5b"
      },
      "source": [
        "## Scale but not augmented\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_set = test_datagen.flow_from_directory(\n",
        "        'data/test_set',\n",
        "        target_size=(64, 64),\n",
        "        batch_size=32,\n",
        "        class_mode='binary')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdvDydtU2d7a"
      },
      "source": [
        "## 2 - CNN construction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WA65n3a4BNV5"
      },
      "source": [
        "###Initialising CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4KwTEn0A-kF"
      },
      "source": [
        "model = tf.keras.models.Sequential()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERYgSPOLBS3E"
      },
      "source": [
        "### Convolution layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5856mVQEfWB"
      },
      "source": [
        "**Conv3D and Conv2D**  \n",
        "Conv2D is used for images. Conv3D is usually used for videos where you have a frame for each time span.  \n",
        "\n",
        "**filters**.  \n",
        "Filters are the number of feature detectors.  \n",
        "\n",
        "**kernel_size**.  \n",
        "kernal is just another name for filter, size means how many row and col your filters going to have, 3 = 3 row by 3 row\n",
        "\n",
        "**input_shape**.  \n",
        "shape of data, height,weight, RGB=3, black white = 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76foXQ-mBSC3"
      },
      "source": [
        "## filters \n",
        "\n",
        "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEy7EDK-GbhM"
      },
      "source": [
        "##Pooling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-gbFjDWG5xX"
      },
      "source": [
        "MaxPool2D and MaxPooling2D is the same, they just undergoes some update but kept the old name as alias\n",
        "\n",
        "The dimension of pooling depends on the dimention of your filter, Conv2D then use "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEPQer2EGiCi"
      },
      "source": [
        "model.add(tf.keras.layers.MaxPool2D)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}